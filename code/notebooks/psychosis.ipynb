{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from scipy.spatial import ConvexHull\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "plt.style.use('ggplot')\n",
    "import pickle\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_graphviz\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAuc(X,y,test_size=0.25,max_depth=None,n_estimators=100,\n",
    "           minsplit=4,FPR=[],TPR=[],VERBOSE=False, USE_ONLY=None):\n",
    "    '''\n",
    "        get AUC given training data X, with target labels y\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    CLASSIFIERS=[DecisionTreeClassifier(max_depth=max_depth, min_samples_split=minsplit),\n",
    "                RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                       max_depth=max_depth,min_samples_split=minsplit),\n",
    "                ExtraTreesClassifier(n_estimators=n_estimators,\n",
    "                                     max_depth=max_depth,min_samples_split=minsplit),\n",
    "                AdaBoostClassifier(n_estimators=n_estimators),\n",
    "                GradientBoostingClassifier(n_estimators=n_estimators,max_depth=max_depth),\n",
    "                svm.SVC(kernel='rbf',gamma='scale',class_weight='balanced',probability=True)]\n",
    "\n",
    "    if USE_ONLY is not None:\n",
    "        if isinstance(USE_ONLY, (list,)):\n",
    "            CLASSIFIERS=[CLASSIFIERS[i] for i in USE_ONLY]\n",
    "        if isinstance(USE_ONLY, (int,)):\n",
    "            CLASSIFIERS=CLASSIFIERS[USE_ONLY]\n",
    "\n",
    "    for clf in CLASSIFIERS:\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred=clf.predict_proba(X_test)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test,y_pred[:,1], pos_label=1)\n",
    "        auc=metrics.auc(fpr, tpr)\n",
    "        if VERBOSE:\n",
    "            print(auc)\n",
    "\n",
    "        FPR=np.append(FPR,fpr)\n",
    "        TPR=np.append(TPR,tpr)\n",
    "    points=np.array([[a[0],a[1]] for a in zip(FPR,TPR)])\n",
    "    hull = ConvexHull(points)\n",
    "    x=np.argsort(points[hull.vertices,:][:,0])\n",
    "    auc=metrics.auc(points[hull.vertices,:][x,0],points[hull.vertices,:][x,1])\n",
    "    return auc,CLASSIFIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConfusion(X,y,test_size=0.25,max_depth=None,n_estimators=100,\n",
    "           minsplit=4,CONFUSION={},VERBOSE=False, USE_ONLY=None,target_names = None):\n",
    "    '''\n",
    "        get AUC given training data X, with target labels y\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    CLASSIFIERS=[DecisionTreeClassifier(max_depth=max_depth, min_samples_split=minsplit),\n",
    "                RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                       max_depth=max_depth,min_samples_split=minsplit),\n",
    "                ExtraTreesClassifier(n_estimators=n_estimators,\n",
    "                                     max_depth=max_depth,min_samples_split=minsplit),\n",
    "                AdaBoostClassifier(n_estimators=n_estimators),\n",
    "                GradientBoostingClassifier(n_estimators=n_estimators,max_depth=max_depth),\n",
    "                svm.SVC(kernel='rbf',gamma='scale',class_weight='balanced',probability=True)]\n",
    "\n",
    "    if USE_ONLY is not None:\n",
    "        if isinstance(USE_ONLY, (list,)):\n",
    "            CLASSIFIERS=[CLASSIFIERS[i] for i in USE_ONLY]\n",
    "        if isinstance(USE_ONLY, (int,)):\n",
    "            CLASSIFIERS=CLASSIFIERS[USE_ONLY]\n",
    "\n",
    "    for clf in CLASSIFIERS:\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred=clf.predict(X_test)\n",
    "        #print(y_test,y_pred)\n",
    "        cmat=confusion_matrix(y_test, y_pred)\n",
    "        acc=accuracy_score(y_test, y_pred)\n",
    "        print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "        CONFUSION[clf]=cmat\n",
    "        \n",
    "        if VERBOSE:\n",
    "            print('Confusion MAtrix:\\n', cmat)\n",
    "            print(' ')\n",
    "            print('Accuracy:', acc)\n",
    "\n",
    "        \n",
    "    return CONFUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('PSYCHO.DAT',header=None,index_col=0,sep='\\s+')\n",
    "df=df[df[1]>0]\n",
    "X=df.loc[:,2:].values\n",
    "y=df.loc[:,1].values.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  schizophrenic       0.87      0.99      0.92       160\n",
      "schizoaffective       0.00      0.00      0.00        15\n",
      "      depressed       0.60      0.46      0.52        13\n",
      "        bipolar       0.50      0.17      0.25         6\n",
      "\n",
      "      micro avg       0.85      0.85      0.85       194\n",
      "      macro avg       0.49      0.40      0.42       194\n",
      "   weighted avg       0.77      0.85      0.80       194\n",
      "\n",
      "Confusion MAtrix:\n",
      " [[158   0   2   0]\n",
      " [ 14   0   1   0]\n",
      " [  6   0   6   1]\n",
      " [  4   0   1   1]]\n",
      " \n",
      "Accuracy: 0.8505154639175257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishanu/.local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ishanu/.local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ishanu/.local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "CONF=getConfusion(X,y,test_size=0.3,max_depth=None,n_estimators=1000,\n",
    "           minsplit=4,VERBOSE=True, USE_ONLY=[2],target_names=['schizophrenic',\n",
    "                                                               'schizoaffective',\n",
    "                                                               'depressed',\n",
    "                                                               'bipolar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '1', '1', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '2', '1', '1', '1', '1', '2', '2', '1', '1', '3', '1',\n",
       "       '3', '4', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '1',\n",
       "       '1', '1', '2', '1', '1', '1', '2', '4', '1', '0', '0', '2', '1',\n",
       "       '1', '1', '1', '2', '2', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '2', '1', '1', '1', '1', '3', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '2', '1', '1', '1', '2', '1', '1', '1', '3', '3', '3', '3', '3',\n",
       "       '3', '4', '3', '3', '3', '1', '3', '2', '1', '1', '3', '1', '1',\n",
       "       '3', '1', '1', '1', '1', '1', '1', '3', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '1',\n",
       "       '1', '1', '1', '3', '1', '1', '1', '1', '1', '1', '1', '3', '4',\n",
       "       '1', '1', '1', '1', '1', '1', '3', '1', '1', '1', '1', '1', '3',\n",
       "       '3', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '3', '3', '1', '1', '3', '1', '1', '1', '3', '1', '1', '3', '1',\n",
       "       '3', '1', '3', '1', '1', '1', '1', '1', '1', '1', '2', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '4', '4', '4',\n",
       "       '4', '4', '4', '1', '1', '1', '1', '3', '1', '3', '2', '1', '1',\n",
       "       '2', '1', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '3', '1', '1', '1', '1', '1', '3', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '4', '1', '2', '1', '1', '1', '2', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '3',\n",
       "       '1', '1', '3', '1', '1', '1', '1', '4', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '3', '1', '1', '1', '2', '1', '1', '1', '1', '1',\n",
       "       '3', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '1',\n",
       "       '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '4',\n",
       "       '3', '1', '1', '3', '1', '1', '1', '1', '1', '1', '1', '4', '3',\n",
       "       '1', '3', '1', '1', '1', '1', '1', '3', '1', '1', '3', '2', '1',\n",
       "       '1', '1', '1', '1', '1', '3', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '2', '1', '1', '3', '2', '1', '1', '1', '1', '3', '1', '1', '3',\n",
       "       '1', '2', '3', '1', '1', '1', '1', '1', '1', '1', '2', '1', '1',\n",
       "       '3', '1', '1', '1', '1', '1', '2', '1', '1', '1', '1', '2', '1',\n",
       "       '1', '1', '1', '1', '2', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '3', '1', '1', '1', '1', '2', '1', '1', '1', '1', '2',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '1',\n",
       "       '1', '1', '1', '1', '1', '2', '1', '2', '1', '1', '2', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '3', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '4', '1', '1', '1', '1', '1', '2', '1', '0', '0', '1',\n",
       "       '1', '1', '4', '1', '2', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '4', '1', '1', '3', '1', '1', '1', '0', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '2', '1', '1', '2', '1', '1', '1'],\n",
       "      dtype='<U21')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
